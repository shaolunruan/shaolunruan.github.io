
<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Shaolun RUAN (阮劭伦) - Homepage</title>







<!-- <meta property="og:locale" content="en">
<meta property="og:site_name" content="Yi Ren (任意)">
<meta property="og:title" content="Yi Ren (任意)">


  <link rel="canonical" href="https://rayeren.github.io/">
  <meta property="og:url" content="https://rayeren.github.io/"> -->



  <!-- <meta property="og:description" content="Research scientist in Bytedance AI Lab, Speech &amp; Audio Team. Focusing on speech, NLP and music."> -->







  <meta name="google-site-verification" content="K3pCHK6p1ze31BTrFjPJN7nM4NF3St6xnOoggmyb4m0" />

  <meta name="baidu-site-verification" content="code-3qUFycruVb" />
  <!-- https://shaolunruan.github.io -->
  <meta name="baidu-site-verification" content="code-nJWfLF15hZ" />

<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>   

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
<link rel="icon" type="image/png" sizes="32x32" href="assets/images/cherry.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<!-- <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="">Publications</a></li>
            <li class="masthead__menu-item"><a href="">CV</a></li>

            <li class="masthead__menu-item"><a href="">Life</a></li>

        <!-- <img style="width: 40px; text-align: right;" src="assets/images/cherry.png" alt=""> -->

          
            <!-- <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
           -->
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="assets/images/tvcg_pic_shaolun.jpg" class="author__avatar" alt="Yi Ren (任意)">
  </div>

  <div class="author__content">
    <h3 class="author__name">Shaolun RUAN (阮劭伦)</h3>
    <p class="author__bio">SMU VIDA Lab</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Ph.D. candidate in Data Visualization and Human-Computer Interaction.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Singapore</li>
      
      
      
      
        <li><a href="mailto:slruan.2021@phdcs.smu.edu.sg"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
        <!-- <li><a href="https://www.linkedin.com/in/rayeren"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li> -->
      
      
        <!-- <li><a href="https://dblp.org/pid/75/6568-6.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i> DBLP</a></li> -->
      
      
      
      
      
      
        <li><a href="https://github.com/shaolunruan"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?hl=zh-CN&user=7Zqm5VsAAAAJ&inst=14102473421921925766"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-6163-9786"><i class="fa-brands fa-orcid"></i> ORCID</a></li>
        <a id="counter" href="https://www.freecounterstat.com" title="visitor counter"><img width="60px" src="https://counter10.stat.ovh/private/freecounterstat.php?c=tulbsc289sk2gr85uzwxequldd52p9g5" title="visitor counter" alt="visitor counter"></a>

      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:rayeren613@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
        <a href="https://www.linkedin.com/in/rayeren"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
        <a href="https://dblp.org/pid/75/6568-6.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i></a>
      
      
      
      
      
      
        <a href="https://github.com/RayeRen"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=4FA6C0AAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0000-0002-9160-3848"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <h1 style="font-size: 1.7em;">Hi, I'm Shaolun Ruan</h1>
            <p style="font-size: 1.2em;">I am currently a Ph.D. candidate of Computer Science at <span  class="blue-link">Singapore Management
                University</span>, under the supervision of Professor <a
                href="http://yong-wang.org/"  class="blue-link">Yong WANG</a>. Before that,
              I received my bachelor degree from <span href="https://www.uestc.edu.cn/" class="blue-link">University of
                Electronic Science and Technology of China</span> at School of Computer
              Science and Engineering in 2019. From 2020 to 2021, I worked as a Research Assistant at Kent State University, U.S.
              </br>
              </br>
              <!-- To enhance the human ability to read and understand big data,  -->
              I develop novel graphical representations that enable a more effective and smoother analysis using machines. 
              My work focuses on improving the accessibility of complex and abstract domain concepts, leveraging the methods from 
              <b>Data Visualization</b>, <b>Human-computer Interaction</b>, and <b>Quantum Computing</b>.
              <!-- My major research interests include <b>Data Visualization</b>, <b>Human-Computer Interaction</b> and
              <b>Quantum Computing</b>. -->
            </p>

<h1 id="-news">🔥 News</h1>
<ul>
  <li><em>2023.11</em>: 🎉 Our paper was accepted by IEEE TVCG. See you in Florida!</li>
  <li><em>2023.07</em>: 🔥 Awarded the SMU Presidential Doctoral Fellowship.</li>
  <li><em>2023.03</em>: 🎉 Our paper was accepted by EuroVIS'23. See you in Leipzig, Germany!</li>
  <li><em>2022.11</em>: Passed the Ph.D. Qualifying Exam and became a Ph.D. candidate.</li>
  <li><em>2022.07</em>: 🎉 Our paper VACSEN was accepted by IEEE VIS'22! See you in Oklahoma City.</li>
  <li><em>2022.05</em>: Our tutorial was accepted by IEEE QCE'22! See you in Broomfield, Colorado.</li>
  <li><em>2021.11</em>: 🎉 Our paper BatchLens was accepted by IEEE DATE'22 as the 1st author.</li>
  <li><em>2021.09</em>: Admitted as a Ph.D. student by SMU under the supervision of Yong WANG.</li>
  <li><em>2021.07</em>: 🎉 My paper Intercept Graph was accepted by IEEE VIS'21.</li>
  <li><em>2020.11</em>: Updated the IELTS overall bands to 7.0.</li>
  <li><em>2019.07</em>: Graduated from University of Science and Tech. of China and get a Bachelor's degree.</li>
  <li><em>2017.08</em>: Completed Engligh Language Academy at the University of Auckland, New Zealand.  </li>
  <li><em>2017.06</em>: Became a Visiting Student at the Univeristy of Melbourne, Australia.</li>
</ul>

<h1 id="-publications">📝 Publications</h1>
<!-- <h2 id="-speech-synthesis">🎙 Speech Synthesis</h2> -->





<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TVCG</div><img src="assets/images/24-TVCG-QuantumEyes.png" alt="sym" width="300px" height="300px" /></div></div>
<div class="paper-box-text">

    <p><a href="">QuantumEyes: Towards Better Interpretability of Quantum Circuits</a> <br />
<strong>Shaolun Ruan</strong>, Qiang Guan, Paul Griffin, Ying Mao, and Yong Wang<br />
IEEE Transactions on Visualization and Computer Graphics (TVCG 2023).

</p>
    <p><a href="https://arxiv.org/pdf/2311.07980.pdf"><strong>PDF</strong></a> | <a href="https://quantumeyes.github.io/"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20QuantumEyes%20system-blue?label=Demo" /></a></p>
  </div>
</div>




<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TVCG</div><img src="assets/images/VACSEN.png" alt="sym" width="80%" /></div></div>
<div class="paper-box-text">

    <p><a href="">VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing</a> <br />
<strong>Shaolun Ruan</strong>, Yong Wang, Weiwen Jiang, Ying Mao, Qiang Guan
<br />
IEEE Transactions on Visualization and Computer Graphics (TVCG 2022).

</p>
    <p><a href="https://ieeexplore.ieee.org/document/9904430"><strong>PDF</strong></a> | <a href="https://vacsensystem.github.io/"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20VACSEN%20system-blue?label=Demo" /></a> | <a href="assets/slides/presentation_video-slides.pdf"><img  src="https://www.gstatic.com/images/branding/product/1x/slides_2020q4_48dp.png" alt="" aria-hidden="true" role="presentation" style="width:25px"></a></p>
  </div>
</div>




<div class="paper-box"><div class="paper-box-image"><div><div class="badge">EuroVis 2023</div><img src="assets/images/venus.png" alt="sym" width="80%" /></div></div>
<div class="paper-box-text">

    <p><a href="">VENUS: A Geometrical Representation for Quantum State Visualization</a> <br />
<strong>Shaolun Ruan</strong>, Ribo Yuan, Qiang Guan, Yanna Lin, Ying Mao, Weiwen Jiang, Zhepeng Wang, Wei Xu, Yong Wang
<br />
Computer Graphics Forum (Proceedings of EuroVis 2023).

</p>
    <p><a href="https://arxiv.org/pdf/2303.08366.pdf"><strong>PDF</strong></a> | <a href="https://github.com/violet-source/venus-page.github.io"><img src="https://img.shields.io/badge/%20VENUS-blue?label=Source" /></a> | <a href="assets/slides/EuroVis23-Shaolun Ruan.pdf"><img  src="https://www.gstatic.com/images/branding/product/1x/slides_2020q4_48dp.png" alt="" aria-hidden="true" role="presentation" style="width:25px"></a></p>
  </div>
</div>



<div class="paper-box"><div class="paper-box-image"><div><div class="badge">VIS 2021</div><img src="assets/images/intercept.png" alt="sym" width="80%" /></div></div>
<div class="paper-box-text">

    <p><a href="">Intercept Graph: An Interactive Radial Visualization for Comparison of State Changes </a> (short paper)
    <br />
<strong>Shaolun Ruan</strong>, Yong Wang, Qiang Guan
<br />
Proceedings of IEEE VIS (VIS 2021).

</p>
    <p><a href="https://ieeexplore.ieee.org/abstract/document/9623307"><strong>PDF</strong></a> </p>
  </div>
</div>



<div class="paper-box"><div class="paper-box-image"><div><div class="badge">DATE 2022</div><img src="assets/images/batch.png" alt="sym" width="80%" /></div></div>
<div class="paper-box-text">

    <p><a href="">BatchLens: A Visualization Approach for Analyzing Batch Jobs in Cloud Systems
    </a> <br />
<strong>Shaolun Ruan</strong>, Yong Wang, Hailong Jiang, Weijia Xu and Qiang Guan
<br />
Proceedings of Design, Automation and Test in Europe Conference (DATE 2022).

</p>
<p><a href="https://dl.acm.org/doi/abs/10.5555/3539845.3539878"><strong>PDF</strong></a> </p>
</div>
</div>



<div class="paper-box"><div class="paper-box-image"><div><div class="badge">PRDC 2023</div><img src="assets/images/prdc.png" alt="sym" width="80%" /></div></div>
<div class="paper-box-text">

    <p><a href="">Visilience: An Interactive Visualization Framework for Resilience Analysis using Control-Flow Graph
    </a> <br />
    Hailong Jiang&#42;, <strong>Shaolun Ruan&#42;</strong>, Bo Fang, Yong Wang, and Qiang Guan
<br />
Proceedings of IEEE PRDC 2023.

</p>
<p><a href="https://shaolun-ruan.com/assets/Visilience_Camera_ready_PRDC23_.pdf"><strong>PDF</strong></a> </p>
</div>
</div>






<!-- 
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICML 2023</code> <a href="https://text-to-audio.github.io/paper.pdf">Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</a>, Rongjie Huang, Jiawei Huang, Dongchao Yang, <strong>Yi Ren</strong>, Luping liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training</a>, Zhenhui Ye, Rongjie Huang, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Xiang Yin and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models</a>, Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, <strong>Yi Ren</strong> and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Revisiting and Incorporating GAN and Diffusion Models in High-Fidelity Speech Synthesis</a>, Rongjie Huang, <strong>Yi Ren</strong>, Ziyue Jiang, Chenye Cui, Jinglin Liu and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech</a>, Rongjie Huang, Chunlei Zhang, <strong>Yi Ren</strong>, Zhou Zhao and Dong Yu</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=SbR9mpTuBn">Bag of Tricks for Unsupervised Text-to-Speech</a>, <strong>Yi Ren</strong>, Chen Zhang, Shuicheng Yan</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech</a>, Ziyue Jiang, Zhe Su, Zhou Zhao, Qian Yang, <strong>Yi Ren</strong>, Jinglin Liu, Zhenhui Ye <a href="https://github.com/Zain-Jiang/Dict-TTS"><img src="https://img.shields.io/github/stars/Zain-Jiang/Dict-TTS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech</a>, Rongjie Huang, <strong>Yi Ren</strong>, Jinglin Liu, Chenye Cui, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus</a>, Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, <strong>Yi Ren</strong>, Jinzheng He, Rongjie Huang, Jieming Zhu, Xiao Chen, Zhou Zhao, <em>(Datasets and Benchmarks Track)</em> <a href="https://github.com/M4Singer/M4Singer"><img src="https://img.shields.io/github/stars/M4Singer/M4Singer?style=social&amp;label=Dataset+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech</a>, Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, <strong>Yi Ren</strong>, <a href="https://github.com/Rongjiehuang/ProDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/ProDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="https://arxiv.org/abs/2110.07468">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</a>, Rongjie Huang, Chenye Cui, Chen Feiayng, <strong>Yi Ren</strong>, Jinglin Liu, Zhou Zhao, Baoxing Huai, Zhefeng Wang</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</a>, Zhenhui Ye, Zhou Zhao, <strong>Yi Ren</strong>, Fei Wu, <a href="https://github.com/yerfor/SyntaSpeech"><img src="https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <span style="color:red">(Oral)</span> <a href="">EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling</a>, Lichao Zhang, Zhou Zhao, <strong>Yi Ren</strong>, Liqun Deng,</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</a>, Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, <strong>Yi Ren</strong>, Zhou Zhao,  <span style="color:red">(Oral)</span>, <a href="https://github.com/Rongjiehuang/FastDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="">A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</a>, Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, <strong>Yi Ren</strong>, Tao Qin, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13066">Revisiting Over-Smoothness in Text to Speech</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13277">Learning the Beauty in Songs: Neural Singing Voice Beautifier</a>, Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, Zhiying Zhu, Zhou Zhao | <a href="https://github.com/MoonInTheRiver/NeuralSVB"><img src="https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2022</code> <a href="https://prosospeech.github.io/">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech</a>, <strong>Yi Ren</strong>, Ming Lei, Zhiying Huang,  Shiliang Zhang, Qian Chen, Zhijie Yan, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <a href="https://arxiv.org/abs/2106.09317">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</a>, Chenye Cui, <strong>Yi Ren</strong>, Jinglin Liu, Feiyang Chen, Rongjie Huang, Ming Lei and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <span style="color:red">(best student paper award candidate)</span> <a href="https://arxiv.org/abs/2106.08507">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</a>, Kexun Zhang, <strong>Yi Ren</strong>, Changliang Xu and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2021</code> <a href="https://arxiv.org/abs/2012.09547">Denoising Text to Speech with Frame-Level Noise Modeling</a>, Chen Zhang, <strong>Yi Ren</strong>, Xu Tan, Jinglin Liu, Kejun Zhang, Tao Qin, Sheng Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/denoispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2021</code> <a href="https://arxiv.org/pdf/2112.10358">Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus</a>, Rongjie Huang, Feiyang Chen, <strong>Yi Ren</strong>, Jinglin Liu, Chenye Cui, Zhou Zhao <span style="color:red">(Oral)</span></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2021</code> <a href="https://www.ijcai.org/proceedings/2021/527">FedSpeech: Federated Text-to-Speech with Continual Learning</a>, Ziyue Jiang, <strong>Yi Ren</strong>, Ming Lei and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403249">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/deepsinger/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403331">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a>, Jin Xu, Xu Tan, <strong>Yi Ren</strong>, Tao Qin, Jian Li, Sheng Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/lrspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2020</code> <a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3139.pdf">MultiSpeech: Multi-Speaker Text to Speech with Transformer</a>, Mingjian Chen, Xu Tan, <strong>Yi Ren</strong>, Jin Xu, Hao Sun, Sheng Zhao, Tao Qin | <a href="https://speechresearch.github.io/multispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2019</code> <span style="color:red">(Oral)</span> <a href="https://pdfs.semanticscholar.org/9075/a3e6271e5ef4953491488d1776527e632408.pdf">Almost Unsupervised Text to Speech and Automatic Speech Recognition</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu  | <a href="https://speechresearch.github.io/unsuper/"><strong>Project</strong></a></li>
</ul> -->
<!-- 
<h2 id="-talkingface-generation">👄 Talkingface Generation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=YfwMIDhPccD">GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a>, Zhenhui Ye, Ziyue Jiang, <strong>Yi Ren</strong>, Jinglin Liu, Jinzheng He, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://arxiv.org/abs/2107.06831">Parallel and High-Fidelity Text-to-Lip Generation</a>, Jinglin Liu, Zhiying Zhu, <strong>Yi Ren</strong>, Wencan Huang, Baoxing Huai, Nicholas Yuan, Zhou Zhao | <a href="https://github.com/Dianezzy/ParaLip"><img src="https://img.shields.io/github/stars/Dianezzy/ParaLip?style=social&amp;label=ParaLip Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19966">Flow-based Unconstrained Lip to Speech Generation</a>, Jinzheng He, Zhou Zhao, <strong>Yi Ren</strong>, Jinglin Liu, Baoxing Huai, Nicholas Yuan</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <a href="https://dl.acm.org/doi/10.1145/3394171.3413740">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</a>, Jinglin Liu, <strong>Yi Ren</strong>, Zhou Zhao, Chen Zhang, Baoxing Huai, Jing Yuan</li>
</ul>

<h2 id="-machine-translation">📚 Machine Translation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation</a>, Rongjie Huang, Huadai Liu, Xize Cheng, <strong>Yi Ren</strong>, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=UVAmFAtC5ye">TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</a>, Rongjie Huang, Jinglin Liu, Huadai Liu, <strong>Yi Ren</strong>, Lichao Zhang, Jinzheng He, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2006.07926">UWSpeech: Speech to Speech Translation for Unwritten Languages</a>, Chen Zhang, Xu Tan, <strong>Yi Ren</strong>, Tao Qin, Kejun Zhang, Tie-Yan Liu | <a href="https://speechresearch.github.io/uwspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2020</code> <a href="https://www.ijcai.org/Proceedings/2020/0534.pdf">Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation</a>, Jinglin Liu, <strong>Yi Ren</strong>, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao and Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://www.aclweb.org/anthology/2020.acl-main.350">SimulSpeech: End-to-End Simultaneous Speech to Text Translation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, Chen Zhang, Qin Tao, Zhou Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://arxiv.org/abs/2004.10454">A Study of Non-autoregressive Model for Sequence Generation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2019</code> <a href="https://openreview.net/forum?id=S1gUsoR9YX">Multilingual Neural Machine Translation with Knowledge Distillation</a>, Xu Tan, <strong>Yi Ren</strong>, Di He, Tao Qin, Zhou Zhao, Tie-Yan Liu</li>
</ul>

<h2 id="-music-generation">🎼 Music Generation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2012.05168">SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint</a>, Zhonghao Sheng, Kaitao Song, Xu Tan, <strong>Yi Ren</strong>, Wei Ye, Shikun Zhang, Tao Qin</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <span style="color:red">(Oral)</span> <a href="https://dl.acm.org/doi/10.1145/3394171.3413721">PopMAG: Pop Music Accompaniment Generation</a>, <strong>Yi Ren</strong>, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/popmag/"><strong>Project</strong></a></li>
</ul>

<h2 id="-generative-model">🧑‍🎨 Generative Model</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2022</code> <a href="https://openreview.net/forum?id=PlKWVd2yBkY">Pseudo Numerical Methods for Diffusion Models on Manifolds</a>, Luping Liu, <strong>Yi Ren</strong>, Zhijie Lin, Zhou Zhao | <a href="https://github.com/luping-liu/PNDM"><img src="https://img.shields.io/github/stars/luping-liu/PNDM?style=social&amp;label=Code+Stars" alt="" /></a> | <a href="https://paperswithcode.com/sota/image-generation-on-celeba-64x64?p=pseudo-numerical-methods-for-diffusion-models-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pseudo-numerical-methods-for-diffusion-models-1/image-generation-on-celeba-64x64" alt="PWC" /></a></li>
</ul>

<h2 id="others">Others</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">Video-Guided Curriculum Learning for Spoken Video Grounding</a>, Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, <strong>Yi Ren</strong></li>
</ul> -->

<h1 id="-honors-and-awards">🎖 Honors and Awards</h1>
<ul>
  <li style="font-size: 1.15em;"><em>2023.07</em> SMU Presidential Doctoral Fellowship (Top 30%)</li>
  <li style="font-size: 1.15em;"><em>2019.05</em> UESTC SCSE Outstanding Student Award</li>
</ul>

<h1 id="-educations">📖 Educations</h1>
<ul>
  <li style="font-size: 1.15em;"><em>2022.01 - current</em>, Ph.D. Candidate, Singapore Management Univeristy, Singapore.</li>
  <li style="font-size: 1.15em;"><em>2019.07 - 2021.09</em>, Research Assistant, Kent State University, U.S.</li>
  <li style="font-size: 1.15em;"><em>2015.09 - 2019.07</em>, Bachelar's Degree, University of Electronic Science and Technology of China, China.</li>
</ul>

<h1 id="-invited-talks">💬 Invited Talks</h1>
<ul>
  <li style="font-size: 1.15em;"><em>2023.11</em>, "Witness" the New Era of Quantum Computing, HKUST VisLab</a> | <a href="assets/slides/HKUST presentation.pdf">[Slides]</a></li>
</ul>


          </section>
        </div>
      </article>
    </div>




    <div id="footer_container">
      <footer>
        <a id="right_reserved">Shaolun Ruan's Personal Website @All Rights Reserved.</a>

        <a id="modified_time">Last Modified: 22/11/2023 21:15:00</a>
      </footer>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SWFCX99KQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "G-SWFCX99KQZ");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/rayeren/rayeren.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            // var totalCitation = data['citedby']
            // document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
